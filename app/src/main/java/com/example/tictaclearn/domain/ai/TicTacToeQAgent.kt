package com.example.tictaclearn.domain.ai

import android.util.Log
import com.example.tictaclearn.data.datastore.QTable
import com.example.tictaclearn.domain.model.Mood
import kotlin.random.Random

private const val TAG = "TicTacToeQAgent"

/**
 * Clase que implementa el algoritmo Q-Learning para jugar al TicTacToe.
 *
 * @property qTable La tabla de memoria (estado -> valores Q)
 * @property mood El estado de 치nimo actual, que define el par치metro Epsilon.
 * @property learningRate (풤) Tasa de aprendizaje: qu칠 tan r치pido debe ajustarse el valor Q.
 * @property discountFactor (풥) Factor de descuento: importancia de las recompensas futuras.
 */
class TicTacToeQAgent(
    private var qTable: QTable,
    private val mood: Mood,
    private val learningRate: Double = 0.1,
    private val discountFactor: Double = 0.9
) {
    // El par치metro epsilon que controla el balance entre exploraci칩n (movimiento aleatorio)
    // y explotaci칩n (movimiento basado en la memoria).
    private val epsilon: Double = mood.epsilon

    // --- L칩gica de Movimiento ---

    /**
     * Elige el mejor movimiento basado en la pol칤tica Epsilon-Greedy.
     * @param state El estado actual del tablero (como String, ej: "X_O_X_O__")
     * @param possibleActions La lista de 칤ndices de celdas vac칤as (ej: [1, 3, 7])
     * @return El 칤ndice de la celda elegida (acci칩n).
     */
    fun selectAction(state: String, possibleActions: List<Int>): Int {
        if (possibleActions.isEmpty()) {
            return -1 // No hay movimientos posibles
        }

        if (Random.nextDouble() < epsilon) {
            // **Exploraci칩n**: Movimiento aleatorio (influenciado por el mood/epsilon)
            val action = possibleActions.random()
            Log.d(TAG, "Exploraci칩n (epsilon=${epsilon}): Elegida acci칩n $action")
            return action
        } else {
            // **Explotaci칩n**: Movimiento basado en la mejor Q-Value
            val action = selectBestAction(state, possibleActions)
            Log.d(TAG, "Explotaci칩n: Elegida acci칩n $action")
            return action
        }
    }

    /**
     * Encuentra la mejor acci칩n (칤ndice de celda) para el estado dado,
     * eligiendo la acci칩n con el valor Q m치s alto.
     */
    private fun selectBestAction(state: String, possibleActions: List<Int>): Int {
        // Inicializa el mejor valor Q con el m칤nimo posible y el mejor 칤ndice como -1
        var maxQValue = Double.NEGATIVE_INFINITY
        var bestAction = -1

        // Asegura que el estado est칠 en la QTable, si no, lo inicializa
        val qValues = qTable[state] ?: initializeState(state)

        for (action in possibleActions) {
            // La acci칩n (칤ndice de la celda) se mapea directamente al 칤ndice en la lista de qValues
            val qValue = qValues[action]

            if (qValue > maxQValue) {
                maxQValue = qValue
                bestAction = action
            }
        }

        // Si no se encuentra un mejor movimiento (por ejemplo, todos son 0.0), se elige el primero
        if (bestAction == -1) {
            bestAction = possibleActions.first()
        }

        return bestAction
    }

    // --- L칩gica de Aprendizaje (Actualizaci칩n de Q-Table) ---

    /**
     * Actualiza la Q-Table usando la f칩rmula de Bellman (la esencia del Q-Learning).
     * Q(s, a) = Q(s, a) + 풤 [r + 풥 * max(Q(s', a')) - Q(s, a)]
     *
     * @param prevState El estado del tablero antes del movimiento.
     * @param action El 칤ndice de la celda elegida (acci칩n).
     * @param reward La recompensa recibida (r).
     * @param newState El estado del tablero despu칠s del movimiento (s').
     */
    fun updateQValue(prevState: String, action: Int, reward: Double, newState: String) {
        // 1. Obtener Q(s, a) actual
        val oldQValues = qTable[prevState] ?: initializeState(prevState)
        val oldQValue = oldQValues[action]

        // 2. Calcular max(Q(s', a')) (El mejor valor Q futuro)
        val maxFutureQ = getMaxQForState(newState) // Usamos la versi칩n p칰blica

        // 3. Aplicar la f칩rmula Q-Learning
        val newQValue = oldQValue + learningRate * (reward + discountFactor * maxFutureQ - oldQValue)

        // 4. Actualizar la Q-Table
        val updatedQValues = oldQValues.toMutableList()
        updatedQValues[action] = newQValue
        qTable = qTable.toMutableMap().apply {
            this[prevState] = updatedQValues
        }

        Log.d(TAG, "Aprendizaje en estado $prevState, acci칩n $action. QViejo=$oldQValue, QNuevo=$newQValue")
    }

    /**
     * Calcula el m치ximo valor Q para un estado dado (max(Q(s', a'))).
     * 游눠 CAMBIO: Hacemos esta funci칩n p칰blica para que TicTacToeGameService la use en saveAiMemory.
     */
    fun getMaxQForState(state: String): Double {
        // Si el estado no est치 en la tabla, lo inicializamos y el max Q es 0.0
        val qValues = qTable[state] ?: initializeState(state)
        return qValues.maxOrNull() ?: 0.0
    }

    /**
     * Inicializa los valores Q para un nuevo estado (tablero).
     * @param state El estado del tablero.
     * @return Una lista de 9 Doubles (0.0), uno por cada celda posible.
     */
    private fun initializeState(state: String): List<Double> {
        val newQValues = List(9) { 0.0 }
        qTable = qTable.toMutableMap().apply {
            this[state] = newQValues
        }
        Log.d(TAG, "Inicializado nuevo estado: $state")
        return newQValues
    }

    // --- Getter de la Q-Table ---

    /**
     * Devuelve la Q-Table actual, incluyendo todas las actualizaciones de aprendizaje.
     */
    fun getCurrentQTable(): QTable = qTable
}