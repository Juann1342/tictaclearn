package com.example.tictaclearn.data.repository

import com.example.tictaclearn.data.datastore.AiMemoryDataStoreManager
import com.example.tictaclearn.data.datastore.MoodDataStoreManager
import com.example.tictaclearn.data.datastore.QTable
import com.example.tictaclearn.domain.model.Board
import com.example.tictaclearn.domain.model.GameResult
import com.example.tictaclearn.domain.model.Mood
import com.example.tictaclearn.domain.model.Player
import com.example.tictaclearn.domain.repository.AIEngineRepository
import com.example.tictaclearn.domain.model.checkGameResult // Importamos la funci√≥n de extensi√≥n
import com.example.tictaclearn.domain.model.Reward // Importamos el objeto Reward
import kotlinx.coroutines.sync.Mutex
import kotlinx.coroutines.sync.withLock
import javax.inject.Inject
import javax.inject.Singleton
import kotlin.random.Random

@Singleton
class AIEngineRepositoryImpl @Inject constructor(
    // üí° DataStore para el Mood (Configuraci√≥n)
    private val moodDataStoreManager: MoodDataStoreManager,
    // üí° DataStore para la Q-Table (Memoria de la IA)
    private val aiMemoryDataStoreManager: AiMemoryDataStoreManager
) : AIEngineRepository {

    // Mutex para asegurar que la Q-Table no sea le√≠da y escrita al mismo tiempo
    private val mutex = Mutex()

    // --- Q-LEARNING CONFIGURACI√ìN ---
    private val random = Random.Default
    private var qTable: QTable = emptyMap() // Cache local para la Q-Table

    private companion object {
        // Constantes del algoritmo
        const val LEARNING_RATE_ALPHA = 0.1   // Tasa de Aprendizaje (Alpha)
        const val DISCOUNT_FACTOR_GAMMA = 0.9 // Factor de Descuento (Gamma)
        // Usamos el objeto Reward para las recompensas
        const val BOARD_SIZE = 9 // El n√∫mero total de celdas (acciones posibles)
    }

    // --- HELPER FUNCTIONS ---

    /**
     * Convierte el tablero en una cadena para la Q-Table usando el m√©todo de Board.
     */
    private fun boardToState(board: Board): String {
        // Llama a la funci√≥n definida en GameState.kt
        return board.toStateString()
    }

    /**
     * Encuentra el √≠ndice (0-8) del √∫ltimo movimiento de la IA.
     * La IA siempre es 'O' (Player.AI) seg√∫n GameState.kt.
     * @return El √≠ndice plano (Int) del movimiento de la IA.
     */
    private fun findLastAIMove(previousBoard: Board, currentBoard: Board): Int? {
        val prevCells = previousBoard.cells
        val currCells = currentBoard.cells
        val aiSymbol = Player.AI.symbol // 'O'

        // Iteramos sobre el √≠ndice plano (0-8)
        for (i in 0 until BOARD_SIZE) {
            // La IA se mueve si la celda estaba vac√≠a y ahora tiene su s√≠mbolo
            if (prevCells[i] == ' ' && currCells[i] == aiSymbol) {
                return i // Retorna el √≠ndice plano (la "acci√≥n")
            }
        }
        return null
    }

    // --- IMPLEMENTACI√ìN DE PERSISTENCIA DEL MOOD ---

    override suspend fun getDailyMood(): Mood {
        // Se asume que el m√©todo DataStore Manager devuelve el ID del Mood.
        val moodId = moodDataStoreManager.getMoodId()
        // Usamos la funci√≥n de ayuda en Mood.kt para encontrar el objeto Mood
        return Mood.fromId(moodId) ?: Mood.getDefaultDailyMood()
    }

    override suspend fun saveDailyMood(mood: Mood) {
        moodDataStoreManager.saveMoodId(mood.id)
    }

    // --- IMPLEMENTACI√ìN DEL APRENDIZAJE DE LA IA ---

    /**
     * 1. Decisi√≥n de Movimiento (Epsilon-Greedy)
     */
    override suspend fun getNextMove(board: Board, currentMood: Mood): Int? {
        // Cargar Q-Table si es necesario (la primera vez)
        mutex.withLock {
            if (qTable.isEmpty()) {
                qTable = aiMemoryDataStoreManager.getQTable()
            }
        }

        val state = boardToState(board)
        // Usamos la funci√≥n de Board para obtener los √≠ndices planos disponibles (acciones)
        val emptyActions = board.getAvailablePositions()
        if (emptyActions.isEmpty()) return null

        // L√≥gica Epsilon-Greedy: El Mood (epsilon) decide si explora o explota
        val epsilon = currentMood.epsilon

        if (random.nextDouble() < epsilon) {
            // **EXPLORACI√ìN:** Movimiento aleatorio (√≠ndice plano)
            return emptyActions.randomOrNull()
        } else {
            // **EXPLOTACI√ìN:** Elige el mejor movimiento de la Q-Table
            val qValues = qTable[state] ?: List(BOARD_SIZE) { 0.0 } // La lista de Q-values tiene tama√±o 9

            val bestAction = emptyActions
                .maxByOrNull { actionIndex -> // actionIndex es el √≠ndice plano (0-8)
                    qValues[actionIndex]
                }

            // Si el mejor movimiento seg√∫n la tabla no es nulo, lo retorna.
            return bestAction ?: emptyActions.randomOrNull()
        }
    }

    /**
     * 2. Aprendizaje y Actualizaci√≥n de Memoria (Q-Learning)
     * ¬°Este es el m√©todo que faltaba por descomentar!
     */
    override suspend fun updateMemory(gameHistory: List<Board>) {
        // Bloqueamos con Mutex para asegurar que la Q-Table no se modifique
        // mientras se calcula o se guarda.
        mutex.withLock {
            // Se necesita al menos un estado inicial y un movimiento para aprender
            if (gameHistory.size < 2) return

            // Obtener la Q-Table actual (de la cach√©) y crear una copia mutable
            val currentQTable = qTable.toMutableMap()
            val finalBoard = gameHistory.last()

            // ‚úÖ USAMOS LA FUNCI√ìN DE EXTENSI√ìN:
            val gameResult = finalBoard.checkGameResult()

            // üí° 1. Determinar la recompensa final (R) usando el objeto Reward
            val finalReward = Reward.getReward(gameResult, Player.AI)

            // üí° 2. Iterar el historial de movimientos de la IA de forma inversa
            for (i in gameHistory.size - 2 downTo 0) {
                val state = boardToState(gameHistory[i])
                val nextState = boardToState(gameHistory[i + 1])

                // Solo actualizamos si el movimiento que llev√≥ de 'state' a 'nextState' fue de la IA.
                // actionIndex es el √≠ndice plano (0-8)
                val actionIndex = findLastAIMove(gameHistory[i], gameHistory[i + 1]) ?: continue

                // 3. Obtener Q-Value actual
                val stateQValues = currentQTable[state]?.toMutableList() ?: MutableList(BOARD_SIZE) { 0.0 }
                val currentQValue = stateQValues[actionIndex]

                // 4. Calcular el Valor M√°ximo Futuro (max Q(s', a'))
                // Si el estado siguiente (nextState) es terminal (Win o Draw),
                // el valor futuro es 0.0 porque no hay m√°s movimientos.
                val maxFutureQ = if (gameHistory[i + 1].checkGameResult() != GameResult.Playing) {
                    0.0
                } else {
                    val nextStateQValues = currentQTable[nextState] ?: List(BOARD_SIZE) { 0.0 }
                    nextStateQValues.maxOrNull() ?: 0.0
                }

                // 5. Determinar la recompensa (solo se aplica la final al pen√∫ltimo estado)
                val reward = if (i == gameHistory.size - 2) finalReward else 0.0

                // 6. Aplicar la F√≥rmula Q-Learning
                // Q(s, a) <- Q(s, a) + Œ± * [ R + Œ≥ * max Q(s', a') - Q(s, a) ]
                val newQValue = currentQValue + LEARNING_RATE_ALPHA * (reward + DISCOUNT_FACTOR_GAMMA * maxFutureQ - currentQValue)

                // 7. Actualizar la tabla en la copia local
                stateQValues[actionIndex] = newQValue
                currentQTable[state] = stateQValues
            }

            // 8. Persistir la Q-Table actualizada en DataStore
            aiMemoryDataStoreManager.saveQTable(currentQTable)
            // 9. Actualizamos la cach√© local
            qTable = currentQTable
        }
    }

    /**
     * 3. Gesti√≥n del Estado (Reseteo)
     */
    override suspend fun clearMemory() {
        mutex.withLock {
            qTable = emptyMap()
            aiMemoryDataStoreManager.clearQTable()
        }
        println("Memoria de la IA (Q-Table) borrada.")
    }
}